{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL ARCHITECTURE","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass SupervisedAttentionModule(nn.Module):\n    def __init__(self,num_features ):\n        super(SupervisedAttentionModule, self).__init__()\n        self.conv1 = nn.Conv2d(num_features, 3, 1)\n        self.conv2 = nn.Conv2d(num_features, num_features, 1)\n        self.conv3 = nn.Conv2d(3, num_features, 1)\n    def forward(self, f_in, degraded_1):\n        r_s = self.conv1(f_in)\n        x_s = r_s + degraded_1\n        f_out = self.conv2(f_in)\n        m = torch.sigmoid(self.conv3(x_s))\n        f_out = f_out * m\n        f_out = f_out + f_in\n        return f_out, x_s\n    \nclass ChannelAttentionBlock(nn.Module):\n    def __init__(self, num_features, kernel, reduction):\n        super(ChannelAttentionBlock, self).__init__()\n        self.prelu = nn.PReLU()\n        self.conv0 = nn.Conv2d(num_features, num_features, kernel, padding = 'same')\n        self.conv1 = nn.Conv2d(num_features, num_features, kernel, padding = 'same')\n        self.conv2 = nn.Conv2d(num_features, num_features//reduction, 1)\n        self.conv3 = nn.Conv2d(num_features//reduction, num_features, 1)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU()\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)\n    \n    def forward(self, x):\n        x1 = self.conv0(x)\n        #print(x1.shape)\n        x1 = self.prelu(x1)\n        x1 = self.conv1(x1)\n        #print(x1.shape)\n        x2 = self.global_avg_pooling(x1)\n        #print(x2.shape)\n        x2 = self.conv2(x2)\n        #print(x2.shape)\n        x2 = self.prelu(x2)\n        x2 = self.conv3(x2)\n        #print(x2.shape)\n        x2 = self.sigmoid(x2)\n        x2 = x2*x1\n        #print(x2.shape)\n        #print(x.shape)\n        out = x2+x\n        return out\n        \n        \n\nclass OriginalResolutionBlock(nn.Module):\n    def __init__(self, kernel, reduction, num_cabs = 8, num_features = 96):\n        super(OriginalResolutionBlock, self).__init__()\n        self.cab_list = []\n        for i in range(num_cabs):\n            self.cab_list.append(ChannelAttentionBlock(num_features,kernel,reduction))\n        self.cab_list.append(nn.Conv2d(num_features,num_features,kernel, padding = 'same'))\n        self.orb = nn.Sequential(*self.cab_list)\n    \n    def forward(self,x):\n        out = self.orb(x)\n        out += x\n        return out\n        \n        \n    \n\nclass OriginalResolutionSubNetwork(nn.Module):\n    def __init__(self, num_features, kernel_size, reduction, num_cabs):\n        super(OriginalResolutionSubNetwork, self).__init__()\n        self.orb1 = OriginalResolutionBlock(kernel_size, reduction, num_cabs, num_features)\n        self.orb2 = OriginalResolutionBlock(kernel_size, reduction, num_cabs, num_features)\n        self.orb3 = OriginalResolutionBlock(kernel_size, reduction, num_cabs, num_features)\n        \n        \n        self.csffenc1 = nn.Conv2d(num_features, num_features, 1)\n        self.csffenc2 = nn.Conv2d(num_features + (num_features//reduction), num_features, 1)\n        self.csffenc3 = nn.Conv2d(num_features + 2*(num_features//reduction), num_features, 1)\n         \n        self.csffdec1 = nn.Conv2d(num_features, num_features, 1)\n        self.csffdec2= nn.Conv2d(num_features, num_features, 1)\n        self.csffdec3= nn.Conv2d(num_features+ (num_features//reduction), num_features, 1)\n        \n        self.up1 = nn.Upsample(scale_factor= 2, mode = 'bilinear', align_corners=True)\n        self.up2 = nn.Upsample(scale_factor = 4, mode = 'bilinear', align_corners = True)\n        \n    def forward(self, x , e1, d1, e2, d2, e3, d3):\n        \n        #print(x.shape, e1.shape, d1.shape, e2.shape, d2.shape, e3.shape, d3.shape)\n        \n        x = self.orb1(x)\n        #print(f\"Shape of e1 is {self.csffenc1(e1).shape}, Shape of d1 is {self.csffdec1(d1).shape}, Shape of x is {x.shape}\")\n        x = x + self.csffenc1(e1) + self.csffdec1(d1)\n        x = self.orb2(x)\n        #print(f\"Shape of e2 is {self.csffenc2(e2).shape}, Shape of d2 is {self.csffdec2(d2).shape}, Shape of x is {x.shape}\")\n        x = x + self.up1(self.csffenc2(e2)) + self.up1(self.csffdec2(d2))\n        x = self.orb3(x)\n        #print(f\"Shape of e3 is {self.csffenc3(e3).shape}, Shape of d3 is {self.csffdec3(d3).shape}, Shape of x is {x.shape}\")\n        x = x + self.up2(self.csffenc3(e3)) + self.up2(self.csffdec3(d3))\n        return x\n\nsame = 'same'\nclass Encoder(nn.Module):\n    def __init__(self, num_features, kernel, reduction, recieve_csff_ip = False):\n        super(Encoder, self).__init__()\n        self.encoder_level1 = nn.Sequential(\n            nn.Conv2d(num_features, num_features, kernel, padding = same),\n            nn.ReLU(),\n            nn.Conv2d(num_features, num_features, kernel, padding = same),\n            nn.ReLU()\n        )\n        self.encoder_level2 = nn.Sequential(\n            nn.Conv2d(num_features, num_features + num_features // reduction, kernel, padding = same),\n            nn.ReLU(),\n            nn.Conv2d(num_features + num_features // reduction, num_features + num_features // reduction, kernel, padding = same),\n            nn.ReLU()\n        )\n        self.encoder_level3 = nn.Sequential(\n            nn.Conv2d(num_features + num_features // reduction, num_features + 2 * (num_features // reduction), kernel, padding = same),\n            nn.ReLU(),\n            nn.Conv2d(num_features + 2 * (num_features // reduction), num_features + 2 * (num_features // reduction), kernel, padding = same),\n            nn.ReLU()\n        )\n        self.down12 = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=True)\n        self.down23 = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=True)\n        \n        if recieve_csff_ip:\n            self.csff1_1 = nn.Conv2d(num_features, num_features, 1)\n            self.csff1_2 = nn.Conv2d(num_features, num_features, 1)\n            self.csff2_1 = nn.Conv2d(num_features + num_features // reduction, num_features + num_features // reduction, 1)\n            self.csff2_2 = nn.Conv2d(num_features , num_features + num_features // reduction, 1)\n            self.csff3_1 = nn.Conv2d( num_features + 2 * (num_features // reduction),  num_features + 2 * (num_features // reduction), 1)\n            self.csff3_2 = nn.Conv2d( num_features + 1 * (num_features // reduction),  num_features + 2 * (num_features // reduction), 1)\n\n\n    def forward(self, x, csff_input1_1 = None, csff_input1_2 = None, csff_input2_1 = None, csff_input2_2 = None, csff_input3_1 = None, csff_input3_2 = None):\n        \n        enc1 = self.encoder_level1(x)\n        if csff_input1_1 is not None and csff_input1_2 is not None:\n            #print(f\"Shape of csff_input1_1 is {csff_input1_1.shape}, csff_input1_2 is {csff_input1_2.shape}\")\n            enc1 = enc1 + self.csff1_1(csff_input1_1) + self.csff1_2(csff_input1_2)\n            \n        x = self.down12(enc1)\n        \n        enc2 = self.encoder_level2(x)\n        if csff_input2_1 is not None and csff_input2_2 is not None:\n            #print(f\"Shape of csff_input2_1 is {csff_input2_1.shape}, csff_input2_2 is {csff_input2_2.shape}\")\n            enc2 = enc2 + self.csff2_1(csff_input2_1) + self.csff2_2(csff_input2_2)\n        \n        x = self.down23(enc2)\n        enc3 = self.encoder_level3(x)\n        if csff_input3_1 is not None and csff_input3_2 is not None:\n            enc3 = enc3 + self.csff3_1(csff_input3_1) + self.csff3_2(csff_input3_2)\n            \n        return [enc1, enc2, enc3]\n\n\nclass Decoder(nn.Module):\n    def __init__(self, num_features, kernel, reduction):\n        super(Decoder, self).__init__()\n        self.decoder_level1 = nn.Sequential(\n            nn.Conv2d(num_features, num_features, kernel, padding = same),\n            nn.ReLU(),\n            nn.Conv2d(num_features, num_features, kernel, padding = same),\n            nn.ReLU()\n        )\n        self.decoder_level2 = nn.Sequential(\n            nn.Conv2d(num_features + num_features // reduction, num_features + num_features // reduction, kernel, padding = same),\n            nn.ReLU(),\n            nn.Conv2d(num_features + num_features // reduction, num_features, kernel, padding = same),\n            nn.ReLU()\n        )\n        self.decoder_level3 = nn.Sequential(\n            nn.Conv2d(num_features + 2*(num_features // reduction), num_features + 2*(num_features // reduction), kernel, padding = same),\n            nn.ReLU(),\n            nn.Conv2d(num_features + 2*(num_features // reduction), num_features + 1*(num_features // reduction), kernel, padding = same),\n            nn.ReLU()\n        )\n        self.up32 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.up21 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n    def forward(self, enc1, enc2,enc3):\n        dec3 = self.decoder_level3(enc3)\n        x3 = self.up32(dec3)+enc2\n        dec2 = self.decoder_level2(x3)\n        x2 = self.up21(dec2)+enc1\n        dec1 = self.decoder_level1(x2)\n        return [dec1, dec2, dec3]\n    \n\n\nclass MPRNET(nn.Module):\n    def __init__(self, num_features = 80, kernel_size = 3, reduction =4 , num_cabs = 8 ):\n        super(MPRNET, self).__init__()\n        self.conv_stage1 = nn.Conv2d(3, num_features, kernel_size, padding = 'same')\n        self.conv_stage2 = nn.Conv2d(3, num_features, kernel_size, padding = 'same')\n        self.conv_stage3 = nn.Conv2d(3, num_features, kernel_size, padding = 'same')\n        self.final_conv = nn.Conv2d(num_features, 3, kernel_size, padding = 'same')\n        \n        \n        self.sam_stage1 = SupervisedAttentionModule(num_features)\n        self.sam_stage2 = SupervisedAttentionModule(num_features)\n        \n        self.cab_stage1 = ChannelAttentionBlock(num_features, kernel_size, reduction)\n        self.cab_stage2 = ChannelAttentionBlock(num_features, kernel_size, reduction)\n        self.cab_stage3 = ChannelAttentionBlock(num_features, kernel_size, reduction)\n        \n        self.encoder1 = Encoder(num_features, kernel_size, reduction)\n        self.encoder2 = Encoder(num_features, kernel_size, reduction, True)\n        \n        self.decoder1 = Decoder(num_features, kernel_size, reduction)\n        self.decoder2 = Decoder(num_features, kernel_size, reduction)\n        \n        self.ors_net = OriginalResolutionSubNetwork(num_features, kernel_size, reduction, num_cabs)\n        \n    def forward(self,image):\n        \n        height = image.shape[-2]\n        width = image.shape[-1]\n        \n        left_image = image[:,:,:,0:int(width/2)]\n        right_image = image[:,:,:,int(width/2):width]\n        \n        top_left_image = left_image[:,:,0:int(height/2),:]\n        bottom_left_image = left_image[:,:,int(height/2):height,:]\n        top_right_image = right_image[:,:,0:int(height/2),:]\n        bottom_right_image= right_image[:,:,int(height/2):height,:]\n        \n        x1_tl = self.conv_stage1(top_left_image)\n        x1_tl = self.cab_stage1(x1_tl)\n        enc1_stage1_tl, enc2_stage1_tl, enc3_stage1_tl = self.encoder1(x1_tl)\n        \n        \n        x1_bl = self.conv_stage1(bottom_left_image)\n        x1_bl = self.cab_stage1(x1_bl)\n        enc1_stage1_bl, enc2_stage1_bl, enc3_stage1_bl = self.encoder1(x1_bl)\n        \n        enc1_stage1_left, enc2_stage1_left, enc3_stage1_left = torch.cat((enc1_stage1_tl,enc1_stage1_bl), dim = 2), torch.cat((enc2_stage1_tl,enc2_stage1_bl), dim = 2), torch.cat((enc3_stage1_tl,enc3_stage1_bl), dim = 2)\n        \n        x1_tr = self.conv_stage1(top_right_image)\n        x1_tr = self.cab_stage1(x1_tr)\n        enc1_stage1_tr, enc2_stage1_tr, enc3_stage1_tr = self.encoder1(x1_tr)\n        \n        x1_br = self.conv_stage1(bottom_right_image)\n        x1_br = self.cab_stage1(x1_br)\n        enc1_stage1_br, enc2_stage1_br, enc3_stage1_br = self.encoder1(x1_br)\n        \n        enc1_stage1_right, enc2_stage1_right, enc3_stage1_right = torch.cat((enc1_stage1_tr,enc1_stage1_br), dim = 2), torch.cat((enc2_stage1_tr,enc2_stage1_br), dim = 2), torch.cat((enc3_stage1_tr,enc3_stage1_br), dim = 2)\n        \n        dec1_stage1_left, dec2_stage1_left, dec3_stage1_left = self.decoder1(enc1_stage1_left, enc2_stage1_left, enc3_stage1_left)\n        dec1_stage1_right, dec2_stage1_right, dec3_stage1_right = self.decoder1(enc1_stage1_right, enc2_stage1_right, enc3_stage1_right)\n        \n        f_out_stage1_left, x_s1_left = self.sam_stage1(dec1_stage1_left, left_image)\n        f_out_stage1_right, x_s1_right = self.sam_stage1(dec1_stage1_right, right_image)\n        \n        x_s1 = torch.cat((x_s1_left, x_s1_right), dim = 3)\n        \n        x2_left = self.conv_stage2(left_image)\n        x2_left = self.cab_stage2(x2_left)\n        enc1_stage2_left, enc2_stage2_left, enc3_stage2_left = self.encoder2(x2_left, enc1_stage1_left, dec1_stage1_left, enc2_stage1_left, dec2_stage1_left, enc3_stage1_left, dec3_stage1_left)\n        \n        x2_right = self.conv_stage2(right_image)\n        x2_right = self.cab_stage2(x2_right)\n        enc1_stage2_right, enc2_stage2_right, enc3_stage2_right = self.encoder2(x2_right, enc1_stage1_right, dec1_stage1_right, enc2_stage1_right, dec2_stage1_right, enc3_stage1_right, dec3_stage1_right)\n        \n        enc1_stage2, enc2_stage2, enc3_stage2 = torch.cat((enc1_stage2_left, enc1_stage2_right), dim= 3), torch.cat((enc2_stage2_left, enc2_stage2_right), dim= 3), torch.cat((enc3_stage2_left, enc3_stage2_right), dim= 3)\n        \n        dec1_stage2, dec2_stage2, dec3_stage2 = self.decoder2(enc1_stage2, enc2_stage2, enc3_stage2)\n        \n        f_out_stage2, x_s2 = self.sam_stage2(dec1_stage2, image)\n        \n        x3 = self.conv_stage3(image)\n        x3 = self.cab_stage3(x3)\n        x3 = x3 + f_out_stage2\n        x3 = self.ors_net(x3, enc1_stage2, dec1_stage2, enc2_stage2, dec2_stage2, enc3_stage2, dec3_stage2)\n        x3 = self.final_conv(x3)\n        x_s3 = x3 + image\n        \n        \n        return x_s1, x_s2, x_s3\n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-10-16T02:03:25.965691Z","iopub.execute_input":"2023-10-16T02:03:25.966012Z","iopub.status.idle":"2023-10-16T02:03:30.240742Z","shell.execute_reply.started":"2023-10-16T02:03:25.965985Z","shell.execute_reply":"2023-10-16T02:03:30.239680Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = \"/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/Gopro/train\"\nVAL_DIR = \"/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/Gopro/test\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nimport os\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\n\nto_tensor = transforms.ToTensor()\n\nclass GoProDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.blur_dir = os.path.join(root_dir, 'blur')\n        self.sharp_dir = os.path.join(root_dir, 'sharp')\n        self.image_files = os.listdir(self.blur_dir)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        blur_image_path = os.path.join(self.blur_dir, self.image_files[idx])\n        sharp_image_path = os.path.join(self.sharp_dir, self.image_files[idx])\n\n        blur_image = Image.open(blur_image_path)\n        sharp_image = Image.open(sharp_image_path)\n\n        blur_image = to_tensor(blur_image)\n        sharp_image = to_tensor(sharp_image)\n            \n        \n        \"\"\"hh, ww = tar_img.shape[1], tar_img.shape[2]\n\n        rr     = random.randint(0, hh-ps)\n        cc     = random.randint(0, ww-ps)\n        aug    = random.randint(0, 8)\n\n        # Crop patch\n        inp_img = inp_img[:, rr:rr+ps, cc:cc+ps]\n        tar_img = tar_img[:, rr:rr+ps, cc:cc+ps]\"\"\"\n            \n        patch_size = 256\n        \n        \n        height = blur_image.shape[1]\n        width = blur_image.shape[2]\n        \n        \n        patch_start_y = random.randint(0, height - patch_size)\n        patch_start_x = random.randint(0, width - patch_size)\n        \n        blur_image = blur_image[: , patch_start_y : patch_start_y + patch_size, patch_start_x : patch_start_x + patch_size]\n        sharp_image = sharp_image[: , patch_start_y : patch_start_y + patch_size, patch_start_x : patch_start_x + patch_size]\n        \n\n        return blur_image, sharp_image\n    \ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    \n])\n\ntrain_dataset = GoProDataset(root_dir = TRAIN_DIR, transform=transform)\n\n\nTrainLoader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=16, drop_last=False, pin_memory=True)\n\nval_dataset = GoProDataset(root_dir = VAL_DIR)\n\nValidationLoader = DataLoader(val_dataset, batch_size = 2, shuffle = True, num_workers=8, drop_last=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T02:03:30.242747Z","iopub.execute_input":"2023-10-16T02:03:30.243514Z","iopub.status.idle":"2023-10-16T02:03:31.323430Z","shell.execute_reply.started":"2023-10-16T02:03:30.243480Z","shell.execute_reply":"2023-10-16T02:03:31.322421Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"'''Code for these losses are taken directly from the authors of the paper'''\n\nclass CharbonnierLoss(nn.Module):\n    \"\"\"Charbonnier Loss (L1)\"\"\"\n\n    def __init__(self, eps=1e-3):\n        super(CharbonnierLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, x, y):\n        diff = x - y\n        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n        return loss\n\nclass EdgeLoss(nn.Module):\n    def __init__(self):\n        super(EdgeLoss, self).__init__()\n        k = torch.Tensor([[.05, .25, .4, .25, .05]])\n        self.kernel = torch.matmul(k.t(),k).unsqueeze(0).repeat(3,1,1,1)\n        if torch.cuda.is_available():\n            self.kernel = self.kernel.cuda()\n        self.loss = CharbonnierLoss()\n\n    def conv_gauss(self, img):\n        n_channels, _, kw, kh = self.kernel.shape\n        img = F.pad(img, (kw//2, kh//2, kw//2, kh//2), mode='replicate')\n        return F.conv2d(img, self.kernel, groups=n_channels)\n\n    def laplacian_kernel(self, current):\n        filtered    = self.conv_gauss(current)    # filter\n        down        = filtered[:,:,::2,::2]       # downsample\n        new_filter  = torch.zeros_like(filtered)\n        new_filter[:,:,::2,::2] = down*4          # upsample\n        filtered    = self.conv_gauss(new_filter) # filter\n        diff = current - filtered\n        return diff\n\n    def forward(self, x, y):\n        loss = self.loss(self.laplacian_kernel(x), self.laplacian_kernel(y))\n        return loss\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-10-16T02:03:31.325120Z","iopub.execute_input":"2023-10-16T02:03:31.325819Z","iopub.status.idle":"2023-10-16T02:03:31.336314Z","shell.execute_reply.started":"2023-10-16T02:03:31.325784Z","shell.execute_reply":"2023-10-16T02:03:31.335414Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport random\nimport time\n\nimport torch\n\ndef compute_psnr(img1, img2, max_val=1.0):\n    \"\"\"\n    Compute the PSNR (Peak Signal-to-Noise Ratio) between two images after normalizing them.\n    \n    Args:\n    - img1 (torch.Tensor): The first image tensor.\n    - img2 (torch.Tensor): The second image tensor.\n    - max_val (float): The maximum possible pixel value of the images before normalization.\n    \n    Returns:\n    - float: The PSNR value.\n    \"\"\"\n    \n    # Normalize the images to [0, 1]\n    img1 = img1 / max_val\n    img2 = img2 / max_val\n    \n    mse = torch.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 10 * torch.log10(1.0 / mse)\n\n\n\n    \n\nlearning_rate = 2e-4\nweight_decay = 1e-1\ncycle = 8\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#model = MPRNET().to(device)\n\nchar_loss = CharbonnierLoss()\nedge_loss = EdgeLoss()\n\n#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.999),eps=1e-8)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=cycle, eta_min=1e-6)\n\ntrain_losses = []\nval_losses = []\n\nfrom tqdm import tqdm\n\nbest_psnr = 0\nbest_epoch = 0\n\nfor epoch in range(start_epoch, 101):\n    #epoch_start_time = time.time()\n    epoch_loss = 0\n    train_id = 1\n\n    model.train()\n    \n    # Initialize tqdm progress bar\n    pbar = tqdm(total=len(TrainLoader), desc=f\"Epoch {epoch}\")\n    \n    for i, (blur_image, sharp_image) in enumerate(TrainLoader, 0):\n\n        # zero_grad\n        for param in model.parameters():\n            param.grad = None\n\n        blur_image = blur_image.to(device)\n        sharp_image = sharp_image.to(device)\n\n        restored_images = model(blur_image)\n \n        # Compute loss at each stage\n        loss_char = sum([char_loss(restored_images[j],sharp_image) for j in range(len(restored_images))])\n        loss_edge = sum([edge_loss(restored_images[j],sharp_image) for j in range(len(restored_images))])\n        loss = (loss_char) + (0.05*loss_edge)\n       \n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n        # Update tqdm progress bar\n        pbar.set_postfix(loss=epoch_loss/(i+1))\n        pbar.update(1)\n\n    #### Evaluation ####\n    \n    model.eval()\n    psnr_val_rgb = []\n    for ii, (blur_image, sharp_image) in enumerate(ValidationLoader, 0):\n        blur_image = blur_image.to(device)\n        sharp_image = sharp_image.to(device)\n\n        with torch.no_grad():\n            restored_images = model(blur_image)\n        restored = restored_images[-1]\n\n        for res, tar in zip(restored, sharp_image):\n            psnr_val_rgb.append(compute_psnr(res, tar))\n\n    psnr_val_rgb = torch.stack(psnr_val_rgb).mean().item()\n\n    if psnr_val_rgb > best_psnr:\n        best_psnr = psnr_val_rgb\n        best_epoch = epoch\n        torch.save({'epoch': epoch, \n                    'state_dict': model.state_dict(),\n                    'optimizer' : optimizer.state_dict()\n                    }, os.path.join(\"/kaggle/working/\",\"model_best.pth\"))\n\n    # Update tqdm progress bar with PSNR\n    pbar.set_postfix(loss=epoch_loss/len(TrainLoader), psnr=psnr_val_rgb)\n    print(\"[epoch %d PSNR: %.4f --- best_epoch %d Best_PSNR %.4f]\" % (epoch, psnr_val_rgb, best_epoch, best_psnr))\n\n    \n\n    scheduler.step()\n    \n    print(\"------------------------------------------------------------------\")\n    print(\"Epoch: {}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch,  epoch_loss, scheduler.get_lr()[0]))\n    print(\"------------------------------------------------------------------\")\n\n    \n\n    # Close the tqdm progress bar\n    pbar.close()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T06:14:52.900666Z","iopub.execute_input":"2023-10-16T06:14:52.900989Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16:   1%|          | 13/1052 [00:14<19:56,  1.15s/it, loss=0.0753]\nEpoch 16: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0921, psnr=26.6]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 16 PSNR: 26.6191 --- best_epoch 16 Best_PSNR 26.6191]\n------------------------------------------------------------------\nEpoch: 16\tLoss: 96.9167\tLearningRate 0.000192\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 1052/1052 [12:58<00:00,  1.35it/s, loss=0.091, psnr=26.8]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 17 PSNR: 26.7636 --- best_epoch 17 Best_PSNR 26.7636]\n------------------------------------------------------------------\nEpoch: 17\tLoss: 95.7264\tLearningRate 0.000171\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0896, psnr=27]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 18 PSNR: 26.9614 --- best_epoch 18 Best_PSNR 26.9614]\n------------------------------------------------------------------\nEpoch: 18\tLoss: 94.2581\tLearningRate 0.000139\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0895, psnr=27.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 19 PSNR: 27.0577 --- best_epoch 19 Best_PSNR 27.0577]\n------------------------------------------------------------------\nEpoch: 19\tLoss: 94.1752\tLearningRate 0.000101\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0885, psnr=27.2]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 20 PSNR: 27.1827 --- best_epoch 20 Best_PSNR 27.1827]\n------------------------------------------------------------------\nEpoch: 20\tLoss: 93.1031\tLearningRate 0.000062\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0868, psnr=27.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 21 PSNR: 27.1442 --- best_epoch 20 Best_PSNR 27.1827]\n------------------------------------------------------------------\nEpoch: 21\tLoss: 91.3613\tLearningRate 0.000030\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0854, psnr=27.4]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 22 PSNR: 27.3970 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 22\tLoss: 89.8188\tLearningRate 0.000009\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0855, psnr=27.4]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 23 PSNR: 27.3678 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 23\tLoss: 89.9294\tLearningRate 0.000200\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0881, psnr=27.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 24 PSNR: 27.0505 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 24\tLoss: 92.7280\tLearningRate 0.000192\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0867, psnr=27.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 25 PSNR: 27.0558 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 25\tLoss: 91.2145\tLearningRate 0.000171\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0879, psnr=27]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 26 PSNR: 26.9676 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 26\tLoss: 92.4737\tLearningRate 0.000139\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0862, psnr=27.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 27 PSNR: 27.0770 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 27\tLoss: 90.6441\tLearningRate 0.000101\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0851, psnr=27.2]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 28 PSNR: 27.1866 --- best_epoch 22 Best_PSNR 27.3970]\n------------------------------------------------------------------\nEpoch: 28\tLoss: 89.5390\tLearningRate 0.000062\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0852, psnr=27.5]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 29 PSNR: 27.4639 --- best_epoch 29 Best_PSNR 27.4639]\n------------------------------------------------------------------\nEpoch: 29\tLoss: 89.6691\tLearningRate 0.000030\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0837, psnr=27.6]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 30 PSNR: 27.5901 --- best_epoch 30 Best_PSNR 27.5901]\n------------------------------------------------------------------\nEpoch: 30\tLoss: 88.0898\tLearningRate 0.000009\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0813, psnr=27.7]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 31 PSNR: 27.6990 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 31\tLoss: 85.5287\tLearningRate 0.000200\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0876, psnr=27.2]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 32 PSNR: 27.2099 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 32\tLoss: 92.1260\tLearningRate 0.000192\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 1052/1052 [12:56<00:00,  1.36it/s, loss=0.086, psnr=27.2]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 33 PSNR: 27.2195 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 33\tLoss: 90.5118\tLearningRate 0.000171\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0862, psnr=27.2]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 34 PSNR: 27.2362 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 34\tLoss: 90.6630\tLearningRate 0.000139\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0859, psnr=27.5]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 35 PSNR: 27.4863 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 35\tLoss: 90.3148\tLearningRate 0.000101\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0839, psnr=27.6]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 36 PSNR: 27.6340 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 36\tLoss: 88.2140\tLearningRate 0.000062\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 1052/1052 [12:56<00:00,  1.35it/s, loss=0.0815, psnr=27.6]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 37 PSNR: 27.5560 --- best_epoch 31 Best_PSNR 27.6990]\n------------------------------------------------------------------\nEpoch: 37\tLoss: 85.7241\tLearningRate 0.000030\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 1052/1052 [12:58<00:00,  1.35it/s, loss=0.0808, psnr=27.9]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 38 PSNR: 27.8535 --- best_epoch 38 Best_PSNR 27.8535]\n------------------------------------------------------------------\nEpoch: 38\tLoss: 85.0278\tLearningRate 0.000009\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0803, psnr=27.9]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 39 PSNR: 27.9261 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 39\tLoss: 84.5061\tLearningRate 0.000200\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 1052/1052 [12:58<00:00,  1.35it/s, loss=0.0856, psnr=27.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 40 PSNR: 27.0541 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 40\tLoss: 90.0916\tLearningRate 0.000192\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0866, psnr=27.5]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 41 PSNR: 27.5349 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 41\tLoss: 91.0735\tLearningRate 0.000171\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0847, psnr=27.4]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 42 PSNR: 27.3722 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 42\tLoss: 89.1218\tLearningRate 0.000139\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0832, psnr=27.7]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 43 PSNR: 27.6609 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 43\tLoss: 87.4779\tLearningRate 0.000101\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0832, psnr=27.7]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 44 PSNR: 27.7382 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 44\tLoss: 87.4839\tLearningRate 0.000062\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0814, psnr=27.9]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 45 PSNR: 27.9120 --- best_epoch 39 Best_PSNR 27.9261]\n------------------------------------------------------------------\nEpoch: 45\tLoss: 85.6396\tLearningRate 0.000030\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46: 100%|██████████| 1052/1052 [12:58<00:00,  1.35it/s, loss=0.08, psnr=28]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 46 PSNR: 27.9767 --- best_epoch 46 Best_PSNR 27.9767]\n------------------------------------------------------------------\nEpoch: 46\tLoss: 84.2066\tLearningRate 0.000009\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0781, psnr=28.1]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 47 PSNR: 28.0776 --- best_epoch 47 Best_PSNR 28.0776]\n------------------------------------------------------------------\nEpoch: 47\tLoss: 82.1090\tLearningRate 0.000200\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0855, psnr=27.7]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 48 PSNR: 27.6670 --- best_epoch 47 Best_PSNR 28.0776]\n------------------------------------------------------------------\nEpoch: 48\tLoss: 89.9392\tLearningRate 0.000192\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0844, psnr=27.6]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 49 PSNR: 27.5885 --- best_epoch 47 Best_PSNR 28.0776]\n------------------------------------------------------------------\nEpoch: 49\tLoss: 88.7875\tLearningRate 0.000171\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50: 100%|██████████| 1052/1052 [12:57<00:00,  1.35it/s, loss=0.0844, psnr=27.7]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 50 PSNR: 27.6612 --- best_epoch 47 Best_PSNR 28.0776]\n------------------------------------------------------------------\nEpoch: 50\tLoss: 88.7729\tLearningRate 0.000139\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51: 100%|██████████| 1052/1052 [12:58<00:00,  1.35it/s, loss=0.0832, psnr=27.6]\n","output_type":"stream"},{"name":"stdout","text":"[epoch 51 PSNR: 27.6425 --- best_epoch 47 Best_PSNR 28.0776]\n------------------------------------------------------------------\nEpoch: 51\tLoss: 87.5285\tLearningRate 0.000101\n------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52:  17%|█▋        | 184/1052 [02:04<09:26,  1.53it/s, loss=0.0806]","output_type":"stream"}]},{"cell_type":"code","source":"def load_checkpoint(model, optimizer, filename):\n    # Note: Input model & optimizer should be pre-defined. This routine only updates their states.\n    start_epoch = 0\n    if os.path.isfile(filename):\n        print(\"=> loading checkpoint '{}'\".format(filename))\n        checkpoint = torch.load(filename)\n        start_epoch = checkpoint['epoch']\n        model.load_state_dict(checkpoint['state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\n                  .format(filename, checkpoint['epoch']))\n    else:\n        print(\"=> no checkpoint found at '{}'\".format(filename))\n\n    return model, optimizer, start_epoch\n# Define the path to your saved checkpoint\ncheckpoint_path = \"/kaggle/working/model_best.pth\"\n\n# Load the checkpoint\nmodel, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T06:14:15.669560Z","iopub.execute_input":"2023-10-16T06:14:15.669935Z","iopub.status.idle":"2023-10-16T06:14:15.951028Z","shell.execute_reply.started":"2023-10-16T06:14:15.669905Z","shell.execute_reply":"2023-10-16T06:14:15.950056Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"=> loading checkpoint '/kaggle/working/model_best.pth'\n=> loaded checkpoint '/kaggle/working/model_best.pth' (epoch 16)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}